---
title: "[DynDNNs] Research Progress: Porting Gemmini::tiled_matmul_auto()"
author: zerogod
date: 2025-08-02 23:03 +0900
categories: [Research, DynDNNs]
tags: [research, gemmini, llama.cpp, DynDNNs]
render_with_liquid: false
---

## ***Abstract***  
This post examines [Gemmini](https://github.com/ucb-bar/gemmini-rocc-tests)’s high-level API function `tiled_matmul_auto()`, which implements systolic-array-based tiled matrix multiplication with automatic tile-size computation.   
We analyze its input parameters and hardware constraints&mdash;such as supported data types and 16-byte alignment requirements&mdash;to ensure correct invocation within the llama.cpp inference engine.

## ***Introduction***  
***Gemmini*** is an open-source, full-stack DNN accelerator generator that produces ASIC designs featuring a parameterizable systolic array, banked scratchpad memory, and DMA subsystems for efficient on-chip data movement.    
Its `tiled_matmul_auto()` function automates the selection of tiling factors to maximize PE utilization, but requires arguments that respect Gemmini’s **element type support and alignment restrictions**.     
When porting this function into llama.cpp&mdash;a lightweight LLM inference engine handling dynamic tensor shapes&mdash;we must first identify the exact argument conditions and fallback behaviors (e.g., CPU emulation) enforced by `tiled_matmul_auto()` under Gemmini’s hardware constraints.

## ***2. Organization***  
1. **Parameters of tiled_matmul_auto()** &mdash; Detailed analysis of each argument and its valid range.  
2. **Testing via QEMU Simulation in llama.cpp** &mdash; Verifying argument handling by initializing and invoking `tiled_matmul_auto()` within the `llama.cpp` test harness. The progress is on [ggml-gemmini](https://github.com/code0-god/ggml-gemmini.git)
3. **Conclusion** &mdash; Summarizing the identified preconditions and fallback pathways.  

---

## ***3. Sections***  
### ***3.1. Parameters of tiled_matmul_auto()***
The prototype of `tiled_matmul_auto()` is:
```cpp
_STATIC void tiled_matmul_auto(size_t dim_I, size_t dim_J, size_t dim_K,
        const elem_t* A, const elem_t* B,
        const void * D, void * C,
        size_t stride_A, size_t stride_B, size_t stride_D, size_t stride_C,
        scale_t A_scale_factor, scale_t B_scale_factor, scale_acc_t D_scale_factor,
        int act, acc_scale_t scale, acc_scale_t bert_scale,
        bool repeating_bias,
        bool transpose_A, bool transpose_B,
        bool full_C, bool low_D,
        uint8_t weightA,
        enum tiled_matmul_type_t tiled_matmul_type) 
```
> [gemmini-rocc-tests](https://github.com/ucb-bar/gemmini-rocc-tests)/include/gemmini.h   

And `tiled_matmul_auto()` is invocked by `tiled_matmul_nn_auto()`:
```cpp
static void tiled_matmul_nn_auto(size_t dim_I, size_t dim_J, size_t dim_K, 
        const elem_t A[dim_I][dim_K], const elem_t B[dim_K][dim_J],
        const void * D, elem_t C[dim_I][dim_J],
        int act, acc_scale_t scale, bool repeating_bias,
        enum tiled_matmul_type_t tiled_matmul_type,
        bool check, char * layer_name)
```
> [gemmini-rocc-tests](https://github.com/ucb-bar/gemmini-rocc-tests)/include/gemmini_nn.h   

`tiled_matmul_auto()` performs the **GEMM**(***GE**neral **M**atrix to **M**atrix multiplication*) operation `C = A × B + D`. When neither `A` nor `B` is transposed, the logical matrix shapes are:
- `A`: I x K 
- `B`: K x J 
- `C`: I x J 
- `D`: 1 x J bias when `repeating_bias == true`, otherwise I x J

All strides are expressed in elements (not bytes); Gemmini converts them internally to byte offsets.

In conclusion, the parameters of `tiled_matmul_auto` means:
```cpp
static void tiled_matmul_auto(
        size_t dim_I, size_t dim_J, size_t dim_K,      // logical matrix sizes
        const elem_t*  A, const elem_t*  B,            // input matrices (8-bit)
        const void*    D,                              // bias matrix (32-bit)
        void*          C,                              // output matrix
        size_t stride_A, size_t stride_B,              // row strides (in elements)
        size_t stride_D, size_t stride_C,
        scale_t     A_scale_factor,                    // integer scale factors
        scale_t     B_scale_factor,
        scale_acc_t D_scale_factor,                   
        int         act,                               // activation function ID
        acc_scale_t scale,                             // post-act scale
        acc_scale_t bert_scale,                        // Softmax / IGELU scale
        bool        repeating_bias,                    // row-broadcast bias
        bool        transpose_A,  bool transpose_B,    // input transposition
        bool        full_C,                            // store C in 32-bit if true
        bool        low_D,                             // bias stored in 8-bit if true
        uint8_t     weightA,                           // experimental flag
        enum tiled_matmul_type_t tiled_matmul_type     // execution path (OS / WS / CPU)
);
```

### ***3.2. Testing via QEMU Simulation in llama.cpp***




