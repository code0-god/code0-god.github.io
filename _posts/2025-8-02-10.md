---
title: "[DynDNNs] Research Progress: Porting Gemmini::tiled_matmul_auto()"
author: zerogod
date: 2025-08-02 23:03 +0900
categories: [Research, DynDNNs]
tags: [research, gemmini, llama.cpp, DynDNNs]
render_with_liquid: false
---

## ***Abstract***  
This post examines [Gemmini](https://github.com/ucb-bar/gemmini-rocc-tests)’s high-level API function `tiled_matmul_auto()`, which implements systolic-array-based tiled matrix multiplication with automatic tile-size computation.   
We analyze its input parameters and hardware constraints&mdash;such as supported data types and 16-byte alignment requirements&mdash;to ensure correct invocation within the llama.cpp inference engine.

## ***Introduction***  
***Gemmini*** is an open-source, full-stack DNN accelerator generator that produces ASIC designs featuring a parameterizable systolic array, banked scratchpad memory, and DMA subsystems for efficient on-chip data movement.    
Its `tiled_matmul_auto()` function automates the selection of tiling factors to maximize PE utilization, but requires arguments that respect Gemmini’s **element type support and alignment restrictions**.     
When porting this function into llama.cpp&mdash;a lightweight LLM inference engine handling dynamic tensor shapes&mdash;we must first identify the exact argument conditions and fallback behaviors (e.g., CPU emulation) enforced by `tiled_matmul_auto()` under Gemmini’s hardware constraints.

## ***2. Organization***  
1. **Parameters of tiled_matmul_auto()** &mdash; Detailed analysis of each argument and its valid range.  
2. **Testing via QEMU Simulation in llama.cpp** &mdash; Verifying argument handling by initializing and invoking `tiled_matmul_auto()` within the `llama.cpp` test harness. The progress is on [ggml-gemmini](https://github.com/code0-god/ggml-gemmini.git)
3. **Conclusion** &mdash; Summarizing the identified preconditions and fallback pathways.  

---

## ***3. Sections***  
### ***3.1. Parameters of tiled_matmul_auto()***
The prototype of `tiled_matmul_auto()` is:
```cpp
_STATIC void tiled_matmul_auto(size_t dim_I, size_t dim_J, size_t dim_K,
        const elem_t* A, const elem_t* B,
        const void * D, void * C,
        size_t stride_A, size_t stride_B, size_t stride_D, size_t stride_C,
        scale_t A_scale_factor, scale_t B_scale_factor, scale_acc_t D_scale_factor,
        int act, acc_scale_t scale, acc_scale_t bert_scale,
        bool repeating_bias,
        bool transpose_A, bool transpose_B,
        bool full_C, bool low_D,
        uint8_t weightA,
        enum tiled_matmul_type_t tiled_matmul_type) 
```
> [gemmini-rocc-tests](https://github.com/ucb-bar/gemmini-rocc-tests)/include/gemmini.h   

And `tiled_matmul_auto()` is invoked by `tiled_matmul_nn_auto()`:
```cpp
static void tiled_matmul_nn_auto(size_t dim_I, size_t dim_J, size_t dim_K, 
        const elem_t A[dim_I][dim_K], const elem_t B[dim_K][dim_J],
        const void * D, elem_t C[dim_I][dim_J],
        int act, acc_scale_t scale, bool repeating_bias,
        enum tiled_matmul_type_t tiled_matmul_type,
        bool check, char * layer_name)
```
> [gemmini-rocc-tests](https://github.com/ucb-bar/gemmini-rocc-tests)/include/gemmini_nn.h   

`tiled_matmul_auto()` performs the **GEMM**(***GE**neral **M**atrix to **M**atrix multiplication*) operation `C = A × B + D`. When neither `A` nor `B` is transposed, the logical matrix shapes are:
- `A`: I x K 
- `B`: K x J 
- `C`: I x J 
- `D`: 1 x J bias when `repeating_bias == true`, otherwise I x J

All stride_* parameters are given in **elements** (not bytes); Gemmini converts them internally to byte offsets.

Finally, The signature of `tiled_matmul_auto()` is:
```cpp
static void tiled_matmul_auto(
    size_t               dim_I,            // Number of rows in A and C
    size_t               dim_J,            // Number of columns in B and C
    size_t               dim_K,            // Shared dimension (A’s cols, B’s rows)
    const elem_t*        A,                // Input matrix A (8-bit elements)
    const elem_t*        B,                // Input matrix B (8-bit elements)
    const void*          D,                // Bias matrix D (accumulator type)
    void*                C,                // Output matrix C
    size_t               stride_A,         // Row stride of A (in elements)
    size_t               stride_B,         // Row stride of B
    size_t               stride_D,         // Row stride of D
    size_t               stride_C,         // Row stride of C
    scale_t              A_scale_factor,   // Quantization scale for A
    scale_t              B_scale_factor,   // Quantization scale for B
    scale_acc_t          D_scale_factor,   // Pre-accumulation scale for D
    int                  act,              // Activation function ID
    acc_scale_t          scale,            // Post-activation scale
    acc_scale_t          bert_scale,       // Additional scale for BERT/IGELU
    bool                 repeating_bias,   // True if D is broadcast per row
    bool                 transpose_A,      // True to transpose A
    bool                 transpose_B,      // True to transpose B
    bool                 full_C,           // True to store C in 32-bit
    bool                 low_D,            // True to treat D as 8-bit
    uint8_t              weightA,          // Experimental flag
    tiled_matmul_type_t  tiled_matmul_type // Execution mode: OS / WS / CPU
);
```
In summary, each parameter defines one aspect of the tiled GEMM:
- `dim_I`, `dim_J`, `dim_K`: Logical matrix dimensions
- `A`, `B`, `D`, `C`: Buffer pointers for input/output 
- `stride_`: Row strides (in elements) for each buffer
- `*_scale_factor`: Quantization and accumulation scales
- `act`, `scale`, `bert_scale`: Activation and post-processing parameters
- Flags (`repeating_bias`, `transpose_*`, `full_C`, `low_D`, `weightA`): Control data layout, precision, and experimental modes
- `tiled_matmul_type`: Selects dataflow mode or CPU fallback

### ***3.2. Testing via QEMU Simulation in llama.cpp***




